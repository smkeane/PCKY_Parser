{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCKY.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94zqF3sJuI3N",
        "outputId": "25bd964f-0e97-4853-bd19-dc79a711c86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ],
      "source": [
        "# CODE BLOCK #1\n",
        "# Summary: makes CFG, parent & child dicts, converts to PCFG type\n",
        "# (def induce) - this makes a pcfg out of a tagged treebank  \n",
        "# (def record_children) - makes parent and child dicts \n",
        "# (def print_probs) - puts rules into a string format so can be made into PCFG\n",
        "\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "from nltk import PCFG\n",
        "\n",
        "# get count of how many x -> and the different types, to get probability\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "treebank_sentences = treebank.parsed_sents()[0:10] # changed to [0:10] for testing rules/data\n",
        "\n",
        "# put in each start of parentheses and pop out last one when parentheses close\n",
        "stack = []\n",
        "count = 0\n",
        "\n",
        "dummy = 0\n",
        "parent_dict = {} # parent: [children]\n",
        "child_dict = {} # child: [parents]\n",
        "\n",
        "# called every time a parent is found\n",
        "def induce(sentence, parent_dict, child_dict):\n",
        "  # go through char in the sentence looking for '('\n",
        "  count = 0\n",
        "  for i in range(0, len(sentence)):\n",
        "    for char in sentence[i]:\n",
        "      char = char.lower()\n",
        "      if sentence[i]=='(':\n",
        "        if i != 0:\n",
        "          parent_dict, child_dict = record_children(i, parent_dict, child_dict, sentence)\n",
        "          count += 1\n",
        "        else: \n",
        "          parent_dict, child_dict = record_children(1, parent_dict, child_dict, sentence)\n",
        "          count += 1\n",
        "  return parent_dict, child_dict\n",
        "    \n",
        "\n",
        "# function to put children in a dictionary when a parent is found\n",
        "def record_children(parent_index, parent_dict, child_dict, test):\n",
        "  # start at where current parent found\n",
        "  test = test[parent_index-1:]\n",
        "  test = test.split()\n",
        "  stack = []\n",
        "  children = [] # for parent_dict\n",
        "  # variable for counting parentheses\n",
        "  count = 0\n",
        "  # variable for seeing if parent ends with a word, counts any non-terminals in the children\n",
        "  non_terminal = -1\n",
        "  # change first digit in range to 1 if TOP before S, otherwise leave as 0\n",
        "  for i in range(0,len(test)):\n",
        "    # go through char in each item\n",
        "    item = test[i]\n",
        "    for char in item:\n",
        "      if char=='(':\n",
        "        # if very start of sentence\n",
        "        if count==0:\n",
        "          # need to add dict with children as keys, use for parsing while parent_dict for printing\n",
        "          parent = item[1:]\n",
        "        # don't add NONE \n",
        "        if parent != '-NONE-':\n",
        "          if parent not in \",.?!:--``\\\"\\\"\\'\\'\":\n",
        "            if parent == '$':\n",
        "              # bc reader hates symbols I guess?\n",
        "              parent = 'dollar_sign'\n",
        "            parent = re.sub('=', '_equals_', parent) \n",
        "            # see if already in parent_dict\n",
        "            if parent_dict.get(parent) == None:\n",
        "              parent_dict[parent] = []\n",
        "              children = []\n",
        "        # add that non-terminal to stack\n",
        "        stack.append(item[1:])\n",
        "        count +=1\n",
        "        non_terminal += 1\n",
        "      # at end of a child\n",
        "      elif char ==')':\n",
        "      # remove non-terminal from stack, have to do -1 if more than 1 non-terminal in stack\n",
        "        count -= 1  \n",
        "        if len(stack) > 1:\n",
        "          child = stack[-1]\n",
        "          \n",
        "           # return parent_dict, child_dict\n",
        "          stack.remove(child)\n",
        "          # when converting into pcfg it breaks w/ -NONE-, so removing it for now since seems to represent a missing subject due to movement in the tree\n",
        "          if child != '-NONE-':\n",
        "            if child not in \",.?!:--``\\\"\\\"\\'\\'\": \n",
        "              if child != \"\\'\\'\":\n",
        "                if child_dict.get(child) == None:\n",
        "                  child_dict[child] = []\n",
        "              # add parent to child's entry if not already there\n",
        "                if parent not in child_dict[child]:\n",
        "                  child_dict[child].append(parent)\n",
        "                if len(stack) == 1:\n",
        "                  children.append(child)\n",
        "        else:\n",
        "          child = stack[0]\n",
        "          stack.remove(child)\n",
        "          if non_terminal == 0:\n",
        "            # go through next item to get the word:\n",
        "            word = ''\n",
        "            for char in test[i]:\n",
        "              if char!=')':\n",
        "                # keep proper nouns uppercase and everything else lower to remove duplicate rules\n",
        "                if parent != \"NNP\":\n",
        "                  char = char.lower()\n",
        "                word += char\n",
        "            # add word to that symbol in the parent dict\n",
        "            # handle words like 're that are part of contractions by removing apostrophe, check for possessive apostrophe first\n",
        "            if word == '\\'':\n",
        "              word = 'pos_apostrophe' # cfg reader doesn't like apostrophes :(\n",
        "            else:\n",
        "              word = re.sub('\\'', '', word)\n",
        "            word = '\\''+word+'\\''\n",
        "            child = word\n",
        "            children.append(word)\n",
        "            if child_dict.get(child) == None:\n",
        "            # figure out controlling for case in the dict entries\n",
        "              child_dict[child] = []\n",
        "            if parent not in child_dict[child]:\n",
        "              child_dict[child].append(parent)\n",
        "  \n",
        "          # make into a tuple so can use Counter later\n",
        "          children = tuple(children)\n",
        "          if parent != '-NONE-':\n",
        "            if parent not in \",.?!:--``\\\"\\\"\\'\\'\":\n",
        "              if children != ():\n",
        "                parent_dict[parent].append(children)\n",
        "          return parent_dict, child_dict\n",
        "      else:\n",
        "        continue\n",
        "      if count==0:\n",
        "        dummy += 1 # do i still use this dummy?\n",
        "\n",
        "# makes a string with all the rules and their probs to be converted into PCFG\n",
        "def print_probs(sample_dict):\n",
        "  final_print = ''\n",
        "  cfg = ''\n",
        "  for element in sample_dict:\n",
        "    # move onto the next element if a parent is punct. or '-NONE-'\n",
        "    if element in \",.?!\":\n",
        "      continue\n",
        "    if element == '-NONE-':\n",
        "      continue\n",
        "    if element == 'PRP$':\n",
        "      element = 'PRP-dollar'\n",
        "    element = re.sub('=', '_equals_', element)\n",
        "    empty_count = 0\n",
        "    total = 0\n",
        "    # go through each of the parent's children\n",
        "    for item in Counter(sample_dict.get(element)):\n",
        "      if len(item) == 1:\n",
        "        if item[0] in \",.?!``\":\n",
        "          continue\n",
        "        if 'NONE' in item[0]:\n",
        "          continue\n",
        "        final_print += str(element) + ' -> ' + str(item[0]) + ' [' + str(Counter(sample_dict.get(element)).get(item)/len(sample_dict.get(element))) + ']\\n'\n",
        "        cfg += str(element) + ' -> ' + str(item[0]) + '\\n'\n",
        "      else:\n",
        "        s = str(element) + ' -> '\n",
        "        for tag in item:\n",
        "          if tag == 'PRP$':\n",
        "            tag = 'PRP-dollar'\n",
        "          if tag == '$':\n",
        "            tag = 'dollar_sign'\n",
        "          tag = re.sub('=', '_equals_', tag)\n",
        "          s += str(tag) + ' '\n",
        "         \n",
        "        cfg += s + '\\n'\n",
        "        s += '[' + str(Counter(sample_dict.get(element)).get(item)/len(sample_dict.get(element))) + ']\\n'\n",
        "\n",
        "        final_print += s       \n",
        "  return final_print, cfg\n",
        "\n",
        "# call our lovely functions :)\n",
        "for line in treebank_sentences:\n",
        "  parent_dict, child_dict = induce(str(line), parent_dict, child_dict)\n",
        "\n",
        "pcfg, cfg = print_probs(parent_dict)\n",
        "# punctuation doesnt wanna work :(\n",
        "old_pcfg = PCFG.fromstring(pcfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE BLOCK #2\n",
        "# Summary: converts PCFG to PCNF\n",
        "# (def remove_hybrids) - removes stuff like (DP -> 'The' NP), but I think this is handled when making the CFG?\n",
        "# (def remove_unit_productions) - removes stuff like (DP -> 'The' | 'a' | 'an'), again I think this is already handled in CB1?\n",
        "# (def TBD) - makes all rules only have two items on the rhs, so A -> B C D becomes A -> X1 D and X1 -> B C\n",
        "\n",
        "import nltk\n",
        "length = len(old_pcfg.productions())\n",
        "# STEP 1: change hybrid rules! No word and non-terminal on RHS, like NP -> ('the' N) etc.\n",
        "def remove_hybrids(grammar, length):\n",
        "  # list to hold rules that are additional to this grammar, like THE -> 'the'\n",
        "  extra_rules = []\n",
        "  # loop through each rule's right side and see if has more than 1 item (terminal or non-terminal)\n",
        "  for i in range(0,length):\n",
        "    rule = grammar.productions()[i].rhs()\n",
        "    rule_len = len(rule)\n",
        "    if rule_len==1:\n",
        "      continue\n",
        "    else:\n",
        "      word_count = 0\n",
        "      nt_count = 0\n",
        "      # loop through each item in right hand of that rule to see if contains both word and non-terminal\n",
        "      for i in range(0, rule_len):\n",
        "        # if there's a word/terminal add to respective count\n",
        "        if 'str' in str(type(rule[i])):\n",
        "          word_count += 1\n",
        "        else:\n",
        "          # it's a non-terminal\n",
        "          nt_count += 1\n",
        "        # if there's at least one of each it's a hybrid\n",
        "        if nt_count > 0 and word_count > 0:\n",
        "          # list for new rule since can't edit specific items in tuple\n",
        "          new_rule = []\n",
        "          # go through each item in righthand side and add to new_rule\n",
        "          for i in range(0, rule_len):\n",
        "            # if it's a word, replace w/ uppercase symbol of it instead\n",
        "            if 'str' in str(type(rule[i])):\n",
        "              new_rule.append(nltk.grammar.Nonterminal(str(rule[i]).upper()))\n",
        "              # write THE -> 'the' rule that has to also be added:\n",
        "              # make list for right side since needs to be in a list\n",
        "              right_list = []\n",
        "              right_list.append(str(rule[i]))\n",
        "              this_rule = [nltk.grammar.Nonterminal(str(rule[i]).upper()), right_list]\n",
        "              extra_rules.append(tuple(this_rule)) \n",
        "            else:\n",
        "              new_rule.append(rule[i])\n",
        "\n",
        "          rule = tuple(new_rule)\n",
        "          # change old rule into new one\n",
        "          grammar.productions()[i] = nltk.grammar.Production(grammar.productions()[i].lhs(), rule)\n",
        "          break\n",
        "\n",
        "  for rule in extra_rules:\n",
        "    grammar.productions().append(nltk.grammar.Production(rule[0], rule[1]))\n",
        "  return extra_rules, grammar\n",
        "\n",
        "# STEP 2 - remove unit productions!\n",
        "def remove_unit_productions(grammar, length):\n",
        "  for i in range(0, length):\n",
        "    rule = grammar.productions()[i].rhs()\n",
        "    rule_len = len(rule)\n",
        "    # see if there's 1 item on right side\n",
        "    if rule_len == 1:\n",
        "      # see if it's a non-terminal\n",
        "      if 'str' in str(type(rule[0])):\n",
        "        continue\n",
        "      else:\n",
        "        # handle rules where that unit goes to words\n",
        "        # find all rules with that unit on left side:\n",
        "        left_list = grammar.productions(lhs=rule[0])\n",
        "        #print('HERES LEFT LIST: ', left_list)\n",
        "        # handle rules where that unit goes to other units (time permitting) # ******LEFT OFF HERE! \n",
        "\n",
        "# make rhs 2 or less NTs - still a WIP...\n",
        "\n",
        "# counter for all the new X1, X2, etc variables\n",
        "new_var_counter = 0\n",
        "def two_on_rhs(pcfg, counter):\n",
        "  #print(toy_pcfg.productions()[5])\n",
        "  #print('LEN: ', len(toy_pcfg.productions()[5].rhs()))\n",
        "  for j in range(0, len(pcfg.productions())):\n",
        "    rule = pcfg.productions()[j]\n",
        "    if len(rule.rhs()) >2:\n",
        "      print('rule longer than 2!')\n",
        "  #rule = pcfg.productions()\n",
        "      lhs = rule.lhs()\n",
        "      first_new_rule = 'X' + str(counter) + ' -> ' + str(rule.rhs()[0]) + ' ' + str(rule.rhs()[1])\n",
        "      # need to check if this rule exists already as something else, will figure out later\n",
        "      print('FIRST NEW RULE: ', first_new_rule)\n",
        "      # make rest of the new rules from there\n",
        "      for i in range(2, len(rule.rhs())):\n",
        "        # last rule brings back original lhs:\n",
        "        if i==(len(rule.rhs())-1):\n",
        "          rule = tuple('X' + str(counter), str(rule.rhs()[i]))\n",
        "          pcfg.productions()[j] = nltk.grammar.Production(pcfg.productions()[j].lhs(), rule)\n",
        "          continue\n",
        "        #print(str(i) + ': ' + str(toy_pcfg.productions()[8].rhs()[i]))\n",
        "        lhs = 'X' + str(counter+1)\n",
        "        s = (str(lhs) + ' -> ' + 'X' + str(counter) + ' ' + str(rule.rhs()[i]))\n",
        "        # add rule to grammar\n",
        "        pcfg.productions().append(s)\n",
        "        \n",
        "        counter += 1\n",
        "\n",
        "\n",
        "extra_rules, new_pcfg = remove_hybrids(old_pcfg, length)\n",
        "remove_unit_productions(new_pcfg, length)\n",
        "#two_on_rhs(new_pcfg, new_var_counter)\n",
        "print(new_pcfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OTue51CPlC0",
        "outputId": "a1703049-a46f-4a44-c915-cd8c6f8c72c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 265 productions (start state = S)\n",
            "    S -> NP-SBJ VP [0.444444]\n",
            "    S -> NP-SBJ-1 VP [0.0555556]\n",
            "    S -> NP-SBJ NP-PRD [0.0555556]\n",
            "    S -> S-TPC-1 NP-SBJ SBAR [0.0555556]\n",
            "    S -> SBAR-TMP NP-SBJ VP [0.0555556]\n",
            "    S -> SBAR VP [0.0555556]\n",
            "    S -> S NP-SBJ PP-LOC [0.0555556]\n",
            "    S -> NP-SBJ-2 VP [0.0555556]\n",
            "    S -> NP-SBJ S [0.0555556]\n",
            "    S -> NP-SBJ SBAR [0.0555556]\n",
            "    S -> NP-SBJ PP-CLR [0.0555556]\n",
            "    NP-SBJ -> NP ADJP [0.0625]\n",
            "    NP-SBJ -> NNP NNP [0.0625]\n",
            "    NP-SBJ -> PP S-CLR [0.0625]\n",
            "    NP-SBJ -> NNS [0.125]\n",
            "    NP-SBJ -> NP NP [0.0625]\n",
            "    NP-SBJ -> PRP [0.125]\n",
            "    NP-SBJ -> NP PP [0.0625]\n",
            "    NP-SBJ -> NP SBAR [0.0625]\n",
            "    NP-SBJ -> DT JJS NNS [0.0625]\n",
            "    NP-SBJ -> DT NNP NN [0.0625]\n",
            "    NP-SBJ -> DT [0.0625]\n",
            "    NP-SBJ -> NN [0.125]\n",
            "    NP-SBJ -> EX [0.0625]\n",
            "    NP -> NNP NNP [0.0701754]\n",
            "    NP -> CD NNS [0.0350877]\n",
            "    NP -> DT NN [0.105263]\n",
            "    NP -> DT JJ NN [0.0526316]\n",
            "    NP -> NN [0.0701754]\n",
            "    NP -> NP NP [0.0701754]\n",
            "    NP -> DT NNP VBG NN [0.0175439]\n",
            "    NP -> JJ NN [0.0350877]\n",
            "    NP -> NNP NNP NNP NNP [0.0175439]\n",
            "    NP -> DT JJ JJ NN [0.0175439]\n",
            "    NP -> NNP NN NNS [0.0175439]\n",
            "    NP -> NP NP PP [0.0175439]\n",
            "    NP -> NN NNS [0.0175439]\n",
            "    NP -> NP RRC [0.0175439]\n",
            "    NP -> NP VP [0.0175439]\n",
            "    NP -> NNS [0.0701754]\n",
            "    NP -> PRP [0.0350877]\n",
            "    NP -> QP NNS [0.0175439]\n",
            "    NP -> DT NN NN [0.0175439]\n",
            "    NP -> DT NNS [0.0175439]\n",
            "    NP -> RB JJ NNS [0.0175439]\n",
            "    NP -> NP S [0.0350877]\n",
            "    NP -> NP NP SBAR [0.0175439]\n",
            "    NP -> ADJP NNP NNP [0.0175439]\n",
            "    NP -> NNP NNS [0.0175439]\n",
            "    NP -> PRP-dollar NN NN NNS [0.0175439]\n",
            "    NP -> CD [0.0175439]\n",
            "    NP -> QP NN [0.0175439]\n",
            "    NP -> NP ADJP [0.0175439]\n",
            "    NP -> NP NNP NNP NNP [0.0175439]\n",
            "    NP -> NN POS [0.0175439]\n",
            "    NP -> NNP [0.0175439]\n",
            "    NP -> DT JJ NNS [0.0175439]\n",
            "    NP -> PRP-dollar NNS [0.0175439]\n",
            "    NNP -> 'Pierre' [0.04]\n",
            "    NNP -> 'Vinken' [0.08]\n",
            "    NNP -> 'Nov.' [0.04]\n",
            "    NNP -> 'Mr.' [0.04]\n",
            "    NNP -> 'Elsevier' [0.04]\n",
            "    NNP -> 'N.V.' [0.04]\n",
            "    NNP -> 'Dutch' [0.04]\n",
            "    NNP -> 'Rudolph' [0.04]\n",
            "    NNP -> 'Agnew' [0.04]\n",
            "    NNP -> 'Consolidated' [0.04]\n",
            "    NNP -> 'Gold' [0.04]\n",
            "    NNP -> 'Fields' [0.04]\n",
            "    NNP -> 'PLC' [0.04]\n",
            "    NNP -> 'Kent' [0.08]\n",
            "    NNP -> 'Lorillard' [0.08]\n",
            "    NNP -> 'Inc.' [0.04]\n",
            "    NNP -> 'Loews' [0.04]\n",
            "    NNP -> 'Corp.' [0.04]\n",
            "    NNP -> 'New' [0.04]\n",
            "    NNP -> 'England' [0.04]\n",
            "    NNP -> 'Journal' [0.04]\n",
            "    NNP -> 'Medicine' [0.04]\n",
            "    ADJP -> NP JJ [0.5]\n",
            "    ADJP -> JJ JJ [0.25]\n",
            "    ADJP -> JJ S [0.25]\n",
            "    CD -> '61' [0.2]\n",
            "    CD -> '29' [0.2]\n",
            "    CD -> '55' [0.2]\n",
            "    CD -> '30' [0.2]\n",
            "    CD -> '1956' [0.2]\n",
            "    NNS -> 'years' [0.210526]\n",
            "    NNS -> 'filters' [0.105263]\n",
            "    NNS -> 'deaths' [0.0526316]\n",
            "    NNS -> 'workers' [0.0526316]\n",
            "    NNS -> 'researchers' [0.105263]\n",
            "    NNS -> 'lungs' [0.0526316]\n",
            "    NNS -> 'exposures' [0.0526316]\n",
            "    NNS -> 'symptoms' [0.0526316]\n",
            "    NNS -> 'decades' [0.0526316]\n",
            "    NNS -> 'cigarettes' [0.0526316]\n",
            "    NNS -> 'findings' [0.0526316]\n",
            "    NNS -> 'results' [0.0526316]\n",
            "    NNS -> 'properties' [0.0526316]\n",
            "    NNS -> 'products' [0.0526316]\n",
            "    JJ -> 'old' [0.166667]\n",
            "    JJ -> 'nonexecutive' [0.111111]\n",
            "    JJ -> 'former' [0.0555556]\n",
            "    JJ -> 'british' [0.0555556]\n",
            "    JJ -> 'industrial' [0.0555556]\n",
            "    JJ -> 'high' [0.0555556]\n",
            "    JJ -> 'resilient' [0.0555556]\n",
            "    JJ -> 'brief' [0.0555556]\n",
            "    JJ -> 'later' [0.0555556]\n",
            "    JJ -> 'new' [0.111111]\n",
            "    JJ -> 'york-based' [0.0555556]\n",
            "    JJ -> 'preliminary' [0.0555556]\n",
            "    JJ -> 'likely' [0.0555556]\n",
            "    JJ -> 'questionable' [0.0555556]\n",
            "    VP -> MD VP [0.03125]\n",
            "    VP -> VB NP PP-CLR NP-TMP [0.03125]\n",
            "    VP -> VBZ NP-PRD [0.0625]\n",
            "    VP -> VBD VP [0.125]\n",
            "    VP -> VBN S [0.03125]\n",
            "    VP -> VBN NP VP [0.03125]\n",
            "    VP -> TO VP [0.0625]\n",
            "    VP -> VB NP [0.03125]\n",
            "    VP -> VBZ PP [0.03125]\n",
            "    VP -> VBN PP [0.03125]\n",
            "    VP -> VBN NP PP-CLR ADVP-TMP [0.03125]\n",
            "    VP -> VBD SBAR [0.0625]\n",
            "    VP -> VBZ ADJP-PRD S S-NOM [0.03125]\n",
            "    VP -> VBZ NP [0.0625]\n",
            "    VP -> VBG S [0.03125]\n",
            "    VP -> VBP PRT ADVP-TMP [0.03125]\n",
            "    VP -> VBG NP PP-LOC-CLR PP-TMP [0.03125]\n",
            "    VP -> VBN NP ADVP-TMP [0.03125]\n",
            "    VP -> VBP PP [0.03125]\n",
            "    VP -> VB NP PP-DIR [0.03125]\n",
            "    VP -> VBP SBAR [0.03125]\n",
            "    VP -> VBG SBAR [0.03125]\n",
            "    VP -> VBD S-NOM [0.03125]\n",
            "    VP -> VBG NP [0.03125]\n",
            "    VP -> VBZ NP-PRD PP-LOC ADVP-TMP [0.03125]\n",
            "    MD -> 'will' [1.0]\n",
            "    VB -> 'join' [0.333333]\n",
            "    VB -> 'make' [0.333333]\n",
            "    VB -> 'bring' [0.333333]\n",
            "    DT -> 'the' [0.35]\n",
            "    DT -> 'a' [0.4]\n",
            "    DT -> 'this' [0.1]\n",
            "    DT -> 'an' [0.05]\n",
            "    DT -> 'any' [0.05]\n",
            "    DT -> 'no' [0.05]\n",
            "    NN -> 'board' [0.0333333]\n",
            "    NN -> 'director' [0.0666667]\n",
            "    NN -> 'chairman' [0.0666667]\n",
            "    NN -> 'group' [0.0666667]\n",
            "    NN -> 'conglomerate' [0.0333333]\n",
            "    NN -> 'form' [0.0333333]\n",
            "    NN -> 'asbestos' [0.133333]\n",
            "    NN -> 'cigarette' [0.0666667]\n",
            "    NN -> 'percentage' [0.0333333]\n",
            "    NN -> 'cancer' [0.0333333]\n",
            "    NN -> 'fiber' [0.0333333]\n",
            "    NN -> 'crocidolite' [0.0666667]\n",
            "    NN -> 'unit' [0.0333333]\n",
            "    NN -> 'micronite' [0.0333333]\n",
            "    NN -> 'year' [0.0333333]\n",
            "    NN -> 'today' [0.0333333]\n",
            "    NN -> 'forum' [0.0333333]\n",
            "    NN -> 'attention' [0.0333333]\n",
            "    NN -> 'problem' [0.0333333]\n",
            "    NN -> 'spokewoman' [0.0333333]\n",
            "    NN -> 'story' [0.0333333]\n",
            "    NN -> 'anyone' [0.0333333]\n",
            "    PP-CLR -> IN NP [0.25]\n",
            "    PP-CLR -> TO NP [0.25]\n",
            "    PP-CLR -> IN SBAR [0.25]\n",
            "    PP-CLR -> IN S-NOM [0.25]\n",
            "    IN -> 'as' [0.04]\n",
            "    IN -> 'of' [0.36]\n",
            "    IN -> 'among' [0.04]\n",
            "    IN -> 'than' [0.08]\n",
            "    IN -> 'ago' [0.12]\n",
            "    IN -> 'once' [0.04]\n",
            "    IN -> 'with' [0.04]\n",
            "    IN -> 'in' [0.16]\n",
            "    IN -> 'although' [0.04]\n",
            "    IN -> 'about' [0.04]\n",
            "    IN -> 'before' [0.04]\n",
            "    NP-TMP -> NNP CD [1.0]\n",
            "    VBZ -> 'is' [0.571429]\n",
            "    VBZ -> 'has' [0.142857]\n",
            "    VBZ -> 'enters' [0.142857]\n",
            "    VBZ -> 'makes' [0.142857]\n",
            "    NP-PRD -> NP PP [0.5]\n",
            "    NP-PRD -> DT JJ NN [0.25]\n",
            "    NP-PRD -> DT NN [0.25]\n",
            "    PP -> IN NP [0.7]\n",
            "    PP -> IN RRC [0.1]\n",
            "    PP -> IN SBAR [0.1]\n",
            "    PP -> TO NP [0.1]\n",
            "    VBG -> 'publishing' [0.2]\n",
            "    VBG -> 'causing' [0.2]\n",
            "    VBG -> 'using' [0.2]\n",
            "    VBG -> 'talking' [0.2]\n",
            "    VBG -> 'having' [0.2]\n",
            "    NP-SBJ-1 -> NP UCP [1.0]\n",
            "    UCP -> ADJP CC PP [1.0]\n",
            "    CC -> 'and' [1.0]\n",
            "    VBD -> 'was' [0.142857]\n",
            "    VBD -> 'reported' [0.142857]\n",
            "    VBD -> 'said' [0.285714]\n",
            "    VBD -> 'stopped' [0.142857]\n",
            "    VBD -> 'were' [0.142857]\n",
            "    VBD -> 'heard' [0.142857]\n",
            "    VBN -> 'named' [0.2]\n",
            "    VBN -> 'used' [0.2]\n",
            "    VBN -> 'caused' [0.2]\n",
            "    VBN -> 'exposed' [0.2]\n",
            "    VBN -> 'reported' [0.2]\n",
            "    S-TPC-1 -> RRC PP-LOC [1.0]\n",
            "    RRC -> ADVP-TMP S-CLR [0.5]\n",
            "    RRC -> VP [0.5]\n",
            "    ADVP-TMP -> RB [0.333333]\n",
            "    ADVP-TMP -> NP IN [0.333333]\n",
            "    ADVP-TMP -> NP JJ [0.166667]\n",
            "    ADVP-TMP -> ADVP SBAR [0.166667]\n",
            "    RB -> 'once' [0.25]\n",
            "    RB -> 'unusually' [0.25]\n",
            "    RB -> 'even' [0.25]\n",
            "    RB -> 'now' [0.25]\n",
            "    S-CLR -> NP-SBJ VP [1.0]\n",
            "    TO -> 'to' [1.0]\n",
            "    PP-LOC -> IN PP [0.666667]\n",
            "    PP-LOC -> IN NP [0.333333]\n",
            "    PRP -> 'it' [0.75]\n",
            "    PRP -> 'we' [0.25]\n",
            "    QP -> RBR IN CD [0.5]\n",
            "    QP -> RBR IN DT [0.5]\n",
            "    RBR -> 'more' [1.0]\n",
            "    SBAR -> S [0.4]\n",
            "    SBAR -> WHNP-1 S [0.2]\n",
            "    SBAR -> WHNP-2 S [0.2]\n",
            "    SBAR -> IN S [0.2]\n",
            "    S-TPC-2 -> NP-SBJ SBAR-TMP [1.0]\n",
            "    ADJP-PRD -> RB JJ [1.0]\n",
            "    SBAR-TMP -> IN S [1.0]\n",
            "    S-NOM -> NP-SBJ SBAR [0.5]\n",
            "    S-NOM -> NP-SBJ VP [0.5]\n",
            "    WHNP-1 -> WDT [1.0]\n",
            "    WDT -> 'that' [1.0]\n",
            "    VBP -> 'show' [0.333333]\n",
            "    VBP -> 'appear' [0.333333]\n",
            "    VBP -> 're' [0.333333]\n",
            "    PRT -> RP [1.0]\n",
            "    RP -> 'up' [1.0]\n",
            "    WHNP-2 -> WDT [1.0]\n",
            "    PP-LOC-CLR -> IN NP [1.0]\n",
            "    PP-TMP -> IN NP [1.0]\n",
            "    SBAR-ADV -> IN S [1.0]\n",
            "    NP-SBJ-2 -> JJ NNS [1.0]\n",
            "    JJS -> 'latest' [1.0]\n",
            "    POS -> 's' [1.0]\n",
            "    PP-DIR -> TO NP [1.0]\n",
            "    ADVP -> NP IN [1.0]\n",
            "    EX -> 'there' [1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE BLOCK #3 - makes CKY grid/data structure for storing each cell's POS and backpointers to other cells\n",
        "\n",
        "import nltk\n",
        "\n",
        "sample = ['Pierre', 'will', 'join', 'the', 'board']\n",
        "\n",
        "cky_grid = []\n",
        "treebank_sentence = (treebank.sents()[0])\n",
        "\n",
        "print(treebank_sentence)\n",
        "# make dictionary(instead of dicts inside list/2d array thingy?) for grid-like CKY system, inner lists are columns i guess\n",
        "# go left to right\n",
        "def array_maker(sentence):\n",
        "  cky_grid = {}\n",
        "  sentence_len = len(sentence)\n",
        "  print('SENTENCE LEN: ', sentence_len)\n",
        "  for j in range(sentence_len+1):\n",
        "    # add a dict for each cell in that column, number it for now\n",
        "    for i in range(1,len(sentence)+1):\n",
        "      cky_grid[(j,i)]=[]\n",
        "  print('HERES THE RESULTING GRID: ', cky_grid)\n",
        "  return cky_grid, sentence_len\n",
        "\n",
        "# i by j grid\n",
        "# go to (i=0,j=0) and get first word's possible POSs from child_dict\n",
        "# 3) go down to (i+1, j+1) and do the same for second word\n",
        "# 4) go up to (i=1, j=0) and put possible parents of first and second word, again found from child_dict\n",
        "# 5) add the backpointers to each parent in (0,1) - FIGURE OUT GOOD DATA STRUCTURE AND/OR MAKE BACKPOINTERS CLASS\n",
        "\n",
        "# for after 2nd column, goes in pattern: (0,1 & 1,4); (0,2 & 2,4); (0,3 & 3,4) for a column w 4 cells in it\n",
        "\n",
        "cky_grid, sentence_len = array_maker(sample)\n",
        "\n",
        "print(cky_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKjvoFE7zbls",
        "outputId": "5c37781c-e277-4cde-c264-0034f88a5e59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "SENTENCE LEN:  5\n",
            "HERES THE RESULTING GRID:  {(0, 1): [], (0, 2): [], (0, 3): [], (0, 4): [], (0, 5): [], (1, 1): [], (1, 2): [], (1, 3): [], (1, 4): [], (1, 5): [], (2, 1): [], (2, 2): [], (2, 3): [], (2, 4): [], (2, 5): [], (3, 1): [], (3, 2): [], (3, 3): [], (3, 4): [], (3, 5): [], (4, 1): [], (4, 2): [], (4, 3): [], (4, 4): [], (4, 5): [], (5, 1): [], (5, 2): [], (5, 3): [], (5, 4): [], (5, 5): []}\n",
            "{(0, 1): [], (0, 2): [], (0, 3): [], (0, 4): [], (0, 5): [], (1, 1): [], (1, 2): [], (1, 3): [], (1, 4): [], (1, 5): [], (2, 1): [], (2, 2): [], (2, 3): [], (2, 4): [], (2, 5): [], (3, 1): [], (3, 2): [], (3, 3): [], (3, 4): [], (3, 5): [], (4, 1): [], (4, 2): [], (4, 3): [], (4, 4): [], (4, 5): [], (5, 1): [], (5, 2): [], (5, 3): [], (5, 4): [], (5, 5): []}\n",
            "{(0, 1): [], (0, 2): [], (0, 3): [], (0, 4): [], (0, 5): [], (1, 1): ['MD'], (1, 2): [], (1, 3): [], (1, 4): [], (1, 5): [], (2, 1): [], (2, 2): [], (2, 3): [], (2, 4): [], (2, 5): [], (3, 1): [], (3, 2): [], (3, 3): [], (3, 4): [], (3, 5): [], (4, 1): [], (4, 2): [], (4, 3): [], (4, 4): [], (4, 5): [], (5, 1): [], (5, 2): [], (5, 3): [], (5, 4): [], (5, 5): [], (0, 0): ['NNP']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE BLOCK 4 - BUILDING UP THE PARSER STEP BY STEP, MOVED BACKPOINTER CLASS HERE!\n",
        "#   i 0 1 2 3 4\n",
        "# j\n",
        "#  1\n",
        "#  2\n",
        "#  3\n",
        "#  4\n",
        "\n",
        "class Backpointer:\n",
        "  'objects for storing backpointer info to non-terminal\\'s child nodes'\n",
        "  def __init__(self, name, l, r):\n",
        "    self.name = name\n",
        "    self.l = l\n",
        "    self.r = r\n",
        "  def display_backpointer(self):\n",
        "    # string for name of non-terminal, tuple for cell indices ['NP', (0,1)]\n",
        "    print(self.name)\n",
        "    print(\"Left child:  \" + str(self.l[0]) + \"\\tPosition: \" + str(self.l[1]))\n",
        "    print(\"Right child: \" + str(self.r[0]) + \"\\tPosition: \" + str(self.r[1]))\n",
        "\n",
        "new_backpointer = Backpointer('S', ['NP', (0,0)], ['VP', (1,1)])\n",
        "new_backpointer.display_backpointer()\n",
        "sample = ['Pierre', 'join', 'the', 'board']\n",
        "\n",
        "def new_cky_parser(sentence, cky_grid):\n",
        "  length = len(sentence)\n",
        "  # go across the columns \n",
        "  for i in range(1, length+1):\n",
        "    # start from bottom of each column and go up:\n",
        "    for j in range(i-1, -1, -1):\n",
        "      #print('j: ',j)\n",
        "      if j == (i-1):\n",
        "        current_word = '\\'' + str(sentence[i-1]) + '\\''\n",
        "        #print('at bottom of column! POS tag is: ', child_dict[current_word])\n",
        "        new_backpointer = Backpointer(child_dict[current_word][0], [],[])\n",
        "        print(cky_grid[(j,i)])\n",
        "        cky_grid[(j,i)].append(new_backpointer)\n",
        "      else:\n",
        "        # start trying out each pivot point, z is number of pivots at that cell:\n",
        "        pivots = i-j-1\n",
        "        print(pivots)\n",
        "        #count = 1\n",
        "        # get coordinates for both cells in pivot\n",
        "        for z in range(1, pivots+1):\n",
        "          print('Pivot #' + str(z) + ': (' + str(j) + ', ' + str(j+z) +') and (' + str(j+z) + ', ' + str(i) +')')\n",
        "          #count += 1\n",
        "          first_coordinates = (j, j+z)\n",
        "          #first_NT = cky_grid[first_coordinates].name\n",
        "\n",
        "          second_coordinates = (j+z, i)\n",
        "          #second_NT = cky_grid[second_coordinates].name\n",
        "          \n",
        "          # check to see if these cells are empty and quit if they are\n",
        "          if cky_grid[first_coordinates] == []:\n",
        "            print('EMPTY 1st NT')\n",
        "            # skip since no possible lhs\n",
        "            continue\n",
        "          # go through each possible NT in that cell\n",
        "           \n",
        "          for backpointer in cky_grid[first_coordinates]:\n",
        "            # get the NT from the cell's backpointer object's 'name'\n",
        "            first_NT = backpointer.name\n",
        "            print('FIRST NONTERMINAL: ',first_NT)\n",
        "            # try each of the second NTs w the first:\n",
        "            if cky_grid[second_coordinates] == []:\n",
        "              print('EMPTY 2nd NT')\n",
        "              # skip since no possible lhs\n",
        "              continue \n",
        "            for backpointer in cky_grid[second_coordinates]:\n",
        "              second_NT = backpointer.name\n",
        "            print('SECOND NONTERMINAL: ',second_NT)\n",
        "            # have to cast into non-terminals then recombine as tuple to check truth\n",
        "            rule_in_q = (nltk.grammar.Nonterminal(str(first_NT)), nltk.grammar.Nonterminal(str(second_NT)))\n",
        "            # go through each rule's rhs to find a match:\n",
        "            for rule in new_pcfg.productions():\n",
        "              #print(rule.rhs(), 'vs', rule_in_q, '\\n')\n",
        "              if rule.rhs() == rule_in_q:\n",
        "                print(\"MATCH FOUND!\")\n",
        "                print('RHS: ', rule.rhs())\n",
        "                another_new_backpointer = Backpointer(rule.lhs(), [str(first_NT), first_coordinates], [str(second_NT), second_coordinates])\n",
        "                cky_grid[(j,i)].append(another_new_backpointer)\n",
        "                \n",
        "          \n",
        "  print(cky_grid)\n",
        "            \n",
        "\n",
        "          #('Next up, check if these two form a rhs in our grammar!')\n",
        "\n",
        "\n",
        "new_cky_parser(sample, cky_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cioTMh9pwdkA",
        "outputId": "495822fd-5f40-44ce-f5c6-2a84967d09f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S\n",
            "Left child:  NP\tPosition: (0, 0)\n",
            "Right child: VP\tPosition: (1, 1)\n",
            "[]\n",
            "[]\n",
            "1\n",
            "Pivot #1: (0, 1) and (1, 2)\n",
            "FIRST NONTERMINAL:  NNP\n",
            "SECOND NONTERMINAL:  VB\n",
            "[]\n",
            "1\n",
            "Pivot #1: (1, 2) and (2, 3)\n",
            "FIRST NONTERMINAL:  VB\n",
            "SECOND NONTERMINAL:  DT\n",
            "2\n",
            "Pivot #1: (0, 1) and (1, 3)\n",
            "FIRST NONTERMINAL:  NNP\n",
            "EMPTY 2nd NT\n",
            "Pivot #2: (0, 2) and (2, 3)\n",
            "EMPTY 1st NT\n",
            "[]\n",
            "1\n",
            "Pivot #1: (2, 3) and (3, 4)\n",
            "FIRST NONTERMINAL:  DT\n",
            "SECOND NONTERMINAL:  NN\n",
            "MATCH FOUND!\n",
            "RHS:  (DT, NN)\n",
            "MATCH FOUND!\n",
            "RHS:  (DT, NN)\n",
            "2\n",
            "Pivot #1: (1, 2) and (2, 4)\n",
            "FIRST NONTERMINAL:  VB\n",
            "SECOND NONTERMINAL:  NP-PRD\n",
            "Pivot #2: (1, 3) and (3, 4)\n",
            "EMPTY 1st NT\n",
            "3\n",
            "Pivot #1: (0, 1) and (1, 4)\n",
            "FIRST NONTERMINAL:  NNP\n",
            "EMPTY 2nd NT\n",
            "Pivot #2: (0, 2) and (2, 4)\n",
            "EMPTY 1st NT\n",
            "Pivot #3: (0, 3) and (3, 4)\n",
            "EMPTY 1st NT\n",
            "{(0, 1): [<__main__.Backpointer object at 0x7efbea46d610>], (0, 2): [], (0, 3): [], (0, 4): [], (0, 5): [], (1, 1): ['MD'], (1, 2): [<__main__.Backpointer object at 0x7efbea46d410>], (1, 3): [], (1, 4): [], (1, 5): [], (2, 1): [], (2, 2): [], (2, 3): [<__main__.Backpointer object at 0x7efbea46d550>], (2, 4): [<__main__.Backpointer object at 0x7efbea46ddd0>, <__main__.Backpointer object at 0x7efbea4fa0d0>], (2, 5): [], (3, 1): [], (3, 2): [], (3, 3): [], (3, 4): [<__main__.Backpointer object at 0x7efbea46dad0>], (3, 5): [], (4, 1): [], (4, 2): [], (4, 3): [], (4, 4): [], (4, 5): [], (5, 1): [], (5, 2): [], (5, 3): [], (5, 4): [], (5, 5): [], (0, 0): ['NNP']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE BLOCK #5 - Makes rules w >2 NTs in rhs into CNF\n",
        "from nltk import PCFG\n",
        "string_cfg = \"\"\"\n",
        "N -> \\'butterfly\\' [0.4]\n",
        "N -> \\'candle\\' [0.2]\n",
        "N -> \\'cake\\' [0.4]\n",
        "VP -> \\'eats\\' [0.3]\n",
        "VP -> \\'runs\\' [0.7]\n",
        "S -> NP VP NNP MD Adv [0.1]\n",
        "S -> NP VP [0.9]\n",
        "NP -> \\'The\\' N [1.0]\n",
        "Z -> A B C D E F G [1.0]\"\"\"\n",
        "toy_pcfg = PCFG.fromstring(string_cfg)\n",
        "# counter for all the new X1, X2, etc variables\n",
        "new_var_counter = 0\n",
        "print(toy_pcfg.productions()[5])\n",
        "print('LEN: ', len(toy_pcfg.productions()[5].rhs()))\n",
        "rule = toy_pcfg.productions()[5]\n",
        "og_lhs = rule.lhs()\n",
        "first_new_rule = 'X' + str(new_var_counter) + ' -> ' + str(rule.rhs()[0]) + ' ' + str(rule.rhs()[1])\n",
        "# need to check if this rule exists already as something else, will figure out if it happens in dev data\n",
        "\n",
        "print('FIRST NEW RULE: ', first_new_rule)\n",
        "# make rest of the new rules from there\n",
        "for i in range(2, len(rule.rhs())):\n",
        "  if i==(len(rule.rhs())-1):\n",
        "    print('FINAL NEW RULE: ' + str(og_lhs) + ' -> ' + 'X' + str(new_var_counter) + ' ' + str(rule.rhs()[i]))\n",
        "    continue\n",
        "  #print(str(i) + ': ' + str(toy_pcfg.productions()[8].rhs()[i]))\n",
        "  lhs = 'X' + str(new_var_counter+1)\n",
        "  print('NEW RULE: ' + str(lhs) + ' -> ' + 'X' + str(new_var_counter) + ' ' + str(rule.rhs()[i]))\n",
        "  # last rule brings back original lhs:\n",
        "  new_var_counter += 1\n",
        "  \n",
        "  \n",
        "# Z -> X1 G\n",
        "# 41 -> X2 F\n",
        "# X -> X3 E\n",
        "# X2 -> X4 D\n",
        "# X1 -> X5 C\n",
        "# X0 -> A B - will want to check if this rule exists already as another dummy rule, so you don't have a repeat dummy rule! Make this rule first!\n",
        "#print(sample[-1])\n",
        "# "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt4GzZDbX9dC",
        "outputId": "fe79a1f6-778d-4a96-dcad-adb76135755a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S -> NP VP NNP MD Adv [0.1]\n",
            "LEN:  5\n",
            "FIRST NEW RULE:  X0 -> NP VP\n",
            "NEW RULE: X1 -> X0 NNP\n",
            "NEW RULE: X2 -> X1 MD\n",
            "FINAL NEW RULE: S -> X2 Adv\n"
          ]
        }
      ]
    }
  ]
}